"""
æ­¤æ¨¡å—å®šä¹‰äº†æ–‡æ¡£ç®¡ç†çš„APIæ¥å£ã€‚

å®ƒåŒ…å«ç”¨äºä¸Šä¼ ã€åˆ é™¤å’Œåˆ—å‡ºæ–‡æ¡£çš„æ¥å£ã€‚

Author: Guo Lijian
"""
from fastapi import APIRouter
from llama_index.readers.file import (
    DocxReader,
    PyMuPDFReader,
    PptxReader,
    PandasExcelReader,
    PandasCSVReader,
    UnstructuredReader
)
from llama_index.core import (
    SimpleDirectoryReader,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.schema import NodeRelationship, RelatedNodeInfo
from dependencies import get_db, get_current_user
from fastapi import Depends, HTTPException, UploadFile, File, Form
from sqlalchemy.orm import Session
from models import User, DocumentRecord, Workspace
from rag_engine import get_vector_index
import os
import shutil

router = APIRouter(prefix="/api/documents", tags=["documents"])

# --- æ–‡ä»¶è§£æå™¨æ˜ å°„å·¥å‚ ---
def get_file_extractors():
    # å°è¯•åˆå§‹åŒ– UnstructuredReader (ç”¨äºå¤æ‚æ ¼å¼)
    try:
        unstructured_reader = UnstructuredReader()
    except Exception:
        print("âš ï¸ Warning: UnstructuredReader åˆå§‹åŒ–å¤±è´¥ï¼Œéƒ¨åˆ†æ—§æ ¼å¼å¯èƒ½æ— æ³•è§£æã€‚è¯· pip install unstructured")
        unstructured_reader = None

    # 1. æ˜¾å¼æŒ‡å®šéœ€è¦ç‰¹æ®Šå¤„ç†çš„æ ¼å¼
    file_extractor = {
        # å¸¸ç”¨åŠå…¬æ–‡æ¡£
        ".pdf": PyMuPDFReader(),
        ".docx": DocxReader(),
        ".pptx": PptxReader(),

        # è¡¨æ ¼ç±» (ä½¿ç”¨ Pandas è¯»å–ï¼Œä¿ç•™ç»“æ„)
        ".xlsx": PandasExcelReader(pandas_config={"header": 0}),
        ".csv": PandasCSVReader(),
    }

    # 2. å¦‚æœå®‰è£…äº† Unstructuredï¼Œæ”¯æŒæ›´å¤šæ ¼å¼
    if unstructured_reader:
        for ext in [".doc", ".ppt", ".xls", ".pages", ".numbers", ".key", ".eml", ".msg"]:
            file_extractor[ext] = unstructured_reader

    # åƒä¸‡ä¸è¦æŠŠ .txt, .md, .py è®¾ä¸º Noneï¼
    # ç›´æ¥ä¸å†™å®ƒä»¬ï¼ŒSimpleDirectoryReader å°±ä¼šè‡ªåŠ¨ä½¿ç”¨é»˜è®¤çš„ TextReader æ­£å¸¸å¤„ç†ã€‚

    return file_extractor


# === æ–‡ä»¶ä¸Šä¼ æ¥å£ ===
@router.post("/upload")
async def upload_file(
        file: UploadFile = File(...),
        is_public: bool = Form(False),
        current_user: User = Depends(get_current_user),
        db: Session = Depends(get_db)
):
    # æƒé™æ£€æŸ¥
    if is_public:
        if current_user.role != "admin":
            # è¿™é‡Œæ ¹æ®éœ€æ±‚ï¼Œå¦‚æœä½ å…è®¸æ™®é€šç”¨æˆ·ä¼ å…¬å…±æ–‡æ¡£å¯ä»¥å»æ‰è¿™ä¸ªåˆ¤æ–­
            pass
        workspace_id = "global"
    else:
        workspace_id = current_user.department_id
        if not workspace_id:
            raise HTTPException(400, "ä½ è¿˜æ²¡åˆ†é…éƒ¨é—¨ï¼Œæ— æ³•ä¸Šä¼ ")

    # æ„é€ å”¯ä¸€ ID (ç”¨äºå‘é‡åº“å®šä½)
    unique_doc_id = f"{workspace_id}_{file.filename}"

    # 1. æ£€æŸ¥ SQL æ•°æ®åº“æ˜¯å¦å­˜åœ¨åŒåè®°å½•
    existing_doc = db.query(DocumentRecord).filter(
        DocumentRecord.workspace_id == workspace_id,
        DocumentRecord.filename == file.filename,
        DocumentRecord.is_global == is_public
    ).first()

    if existing_doc:
        print(f"ğŸ”„ å‘ç°åŒåæ–‡ä»¶ '{file.filename}'ï¼Œæ­£åœ¨æ‰§è¡Œè¦†ç›–æ“ä½œ...")
        try:
            # 2. åˆ é™¤æ—§çš„å‘é‡æ•°æ®
            index = get_vector_index()
            index.delete_ref_doc(unique_doc_id, delete_from_docstore=True, delete_from_vector_store=True)
            print(f"   - æ—§å‘é‡æ•°æ®å·²æ¸…æ´—: {unique_doc_id}")

            # 3. åˆ é™¤æ—§çš„ SQL è®°å½• (æˆ–è€…ä½ å¯ä»¥é€‰æ‹© updateï¼Œä½† delete å† add æ›´å¹²å‡€)
            db.delete(existing_doc)
            db.commit()  # æäº¤åˆ é™¤
            print(f"   - æ—§æ•°æ®åº“è®°å½•å·²åˆ é™¤")

        except Exception as e:
            print(f"âš ï¸ è¦†ç›–æ—§æ–‡ä»¶å¤±è´¥: {e}")
            # ä¸é˜»æ–­æµç¨‹ï¼Œç»§ç»­å°è¯•ä¸Šä¼ ï¼Œæˆ–è€…é€‰æ‹© raise æŠ¥é”™
            raise HTTPException(500, "è¦†ç›–æ—§æ–‡ä»¶å¤±è´¥")

    # 1. æ–‡ä»¶æ°¸ä¹…ä¿å­˜åˆ°æœåŠ¡å™¨filesç›®å½•ä¸‹workspaceå­ç›®å½•ï¼Œä»¥ä¾¿åç»­ä¸‹è½½æŸ¥çœ‹
    store_dir = f"./files/{workspace_id}"
    if not os.path.exists(store_dir): os.makedirs(store_dir)

    file_ext = os.path.splitext(file.filename)[1].lower()
    file_path = os.path.join(store_dir, file.filename)

    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)

    try:
        print(f"ğŸ“¥ å¤„ç†æ–‡ä»¶: {file.filename} ({file_ext}) -> PGVector")

        # 2. ğŸ”¥ è¯»å–æ–‡ä»¶ (ä½¿ç”¨ file_extractor)
        # è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨æ ¹æ®åç¼€åè°ƒç”¨ Pandas, Unstructured ç­‰è§£æå™¨
        loader = SimpleDirectoryReader(
            input_files=[file_path],
            file_extractor=get_file_extractors()  # <--- å…³é”®æ³¨å…¥
        )
        documents = loader.load_data()

        if not documents:
            raise HTTPException(400, "æ–‡ä»¶è§£æä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦å¯è¯»")

        total_text_len = sum([len(d.text.strip()) for d in documents])
        if total_text_len < 10:
            # å¦‚æœè¯»å‡ºæ¥å…¨æ˜¯ç©ºçš„ï¼ŒæŠ›å‡ºè­¦å‘Šæˆ–é”™è¯¯
            # è¿™é‡Œçš„ status_code 400 ä¼šè®©å‰ç«¯ alert é”™è¯¯ä¿¡æ¯
            raise HTTPException(status_code=400,
                                detail="æ— æ³•è¯»å–æ–‡æ¡£æ–‡å­—ã€‚è¯·ç¡®ä¿ä¸Šä¼ çš„æ˜¯ã€æ–‡å­—ç‰ˆPDFã€‘è€Œéã€æ‰«æå›¾ç‰‡ç‰ˆPDFã€‘ã€‚")

        # 3. ğŸ”¥ ç»™ Document æ‰“æ ‡ç­¾ (Metadata)
        is_global_str = "true" if workspace_id == "global" else "false"

        for doc in documents:
            # è¿™é‡Œçš„ doc.id_ æ˜¯ LlamaIndex å†…éƒ¨çš„ IDï¼Œæˆ‘ä»¬æš‚æ—¶ä¸æ”¹å®ƒä¸º unique_doc_id
            # å› ä¸ºä¸€ä¸ª Excel å¯èƒ½è§£æå‡ºå¤šä¸ª Doc (æ¯ä¸ª Sheet ä¸€ä¸ª)ï¼Œå¼ºåˆ¶æ”¹æˆä¸€æ ·ä¼šè¦†ç›–
            # æˆ‘ä»¬ä¸»è¦ä¾èµ– node.ref_doc_id å’Œ metadata æ¥åšåˆ é™¤

            doc.metadata["workspace_id"] = workspace_id
            doc.metadata["file_name"] = file.filename
            doc.metadata["file_key"] = unique_doc_id  # å­˜ä¸€ä¸ªå”¯ä¸€ Key æ–¹ä¾¿åç»­æŸ¥æ‰¾
            doc.metadata["is_global"] = is_global_str
            doc.metadata["uploader_email"] = current_user.email

            # æ’é™¤ä¸è®©å¤§æ¨¡å‹çœ‹åˆ°çš„ Metadata
            doc.excluded_llm_metadata_keys = ["workspace_id", "is_global", "file_key", "uploader_email", "file_name"]
            doc.excluded_embed_metadata_keys = ["workspace_id", "is_global", "file_key", "uploader_email", "file_name"]

        # 4. åˆ‡åˆ†å¹¶æ’å…¥ Nodes
        # chunk_size: æ¯ä¸ªç‰‡æ®µçš„å¤§å° (Tokenæ•°)ã€‚256 çº¦ç­‰äº 300-400 ä¸ªæ±‰å­—ã€‚
        # chunk_overlap: ä¸Šä¸‹æ–‡é‡å ï¼Œé˜²æ­¢åˆ‡æ–­å¥å­ã€‚
        splitter = SentenceSplitter(
            chunk_size=512, # æŠŠåˆ‡ç‰‡æ”¹å¤§ä¸€ç‚¹ï¼Œå¯ä»¥è®©è¯­ä¹‰æ›´è¿è´¯
            chunk_overlap=50
        )
        nodes = splitter.get_nodes_from_documents(documents)

        # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨ relationships å­—å…¸æ¥è®¾ç½®çˆ¶æ–‡æ¡£å…³è”
        # åŸæ¥çš„ node.ref_doc_id = unique_doc_id ä¼šæŠ¥é”™ï¼Œå› ä¸ºå®ƒæ˜¯åªè¯»å±æ€§
        for node in nodes:
            node.relationships[NodeRelationship.SOURCE] = RelatedNodeInfo(node_id=unique_doc_id)

        index = get_vector_index()
        index.insert_nodes(nodes)

        print(f"âœ… æˆåŠŸæ’å…¥ {len(nodes)} ä¸ªç‰‡æ®µ (æ ¼å¼: {file_ext})")

        # 5. å­˜å…¥ SQL ä¸šåŠ¡æ•°æ®åº“
        size_mb = os.path.getsize(file_path) / (1024 * 1024)
        if size_mb < 1:
            size_str = f"{size_mb * 1024:.1f} KB"
        else:
            size_str = f"{size_mb:.2f} MB"

        new_doc = DocumentRecord(
            filename=file.filename,
            file_size=size_str,
            workspace_id=workspace_id,
            is_global=is_public,
            uploader_id=current_user.id
        )
        db.add(new_doc)
        db.commit()

        return {"status": "success", "filename": file.filename}

    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–åº“: {e}")
        raise HTTPException(500, f"æœåŠ¡å™¨ç¼ºå°‘è§£æè¯¥æ ¼å¼çš„ä¾èµ–: {str(e)}")
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"âŒ Upload Error: {e}")
        raise HTTPException(status_code=500, detail=f"è§£æå¤±è´¥: {str(e)}")


# ================= åˆ é™¤æ¥å£ =================
@router.delete("/{doc_id}")
async def delete_document(
        doc_id: int,
        current_user: User = Depends(get_current_user),
        db: Session = Depends(get_db)
):
    doc = db.query(DocumentRecord).filter(DocumentRecord.id == doc_id).first()
    if not doc:
        raise HTTPException(404, "æ–‡æ¡£ä¸å­˜åœ¨")

    can_delete = False
    if current_user.role == "admin":
        can_delete = True
    elif doc.workspace_id == current_user.department_id: # åŒéƒ¨é—¨çš„äººå¯ä»¥åˆ é™¤åŒéƒ¨é—¨çš„æ–‡æ¡£
        can_delete = True

    if not can_delete:
        raise HTTPException(status_code=403, detail="ä½ æ— æƒåˆ é™¤æ­¤æ–‡æ¡£")

    try:
        index = get_vector_index()
        workspace_id = doc.workspace_id
        # æ„é€  ID
        unique_doc_id = f"{workspace_id}_{doc.filename}"

        print(f"ğŸ—‘ï¸ åˆ é™¤å‘é‡å¼•ç”¨: {unique_doc_id}")
        index.delete_ref_doc(unique_doc_id, delete_from_docstore=True, delete_from_vector_store=True)

        db.delete(doc)
        db.commit()

        # åˆ é™¤æ–‡ä»¶
        file_path = f"./files/{workspace_id}/{doc.filename}"
        if os.path.exists(file_path):
            os.remove(file_path)

        return {"status": "success", "msg": "æ–‡æ¡£å·²åˆ é™¤"}

    except Exception as e:
        print(f"Delete Error: {e}")
        raise HTTPException(500, f"åˆ é™¤å¤±è´¥: {str(e)}")


# --- åˆ—è¡¨æ¥å£ (è¡¥å…¨å­—æ®µ) ---
@router.get("")
async def get_documents(
        workspace_id: str,
        db: Session = Depends(get_db),
        current_user: User = Depends(get_current_user)
):
    # 1. ğŸ”¥ ä¿®æ­£ç‚¹ï¼šåŒæ—¶æŸ¥è¯¢ DocumentRecord å¯¹è±¡ å’Œ Workspace.name å­—æ®µ
    query = db.query(DocumentRecord, Workspace.name)

    # 2. ğŸ”¥ ä¿®æ­£ç‚¹ï¼šä½¿ç”¨ func.concat å¤„ç† SQL å­—ç¬¦ä¸²æ‹¼æ¥
    join_condition = DocumentRecord.workspace_id == Workspace.id

    # 3. æ„é€ æŸ¥è¯¢
    if current_user.role == "admin":
        # ç®¡ç†å‘˜ï¼šæŸ¥çœ‹æ‰€æœ‰ï¼Œå·¦è¿æ¥è·å–éƒ¨é—¨åç§°
        docs = query.join(
            Workspace,
            join_condition,
            isouter=True
        ).order_by(DocumentRecord.upload_date.desc()).all()
    else:
        # æ™®é€šç”¨æˆ·ï¼šè¿‡æ»¤éƒ¨é—¨
        docs = query.join(
            Workspace,
            join_condition,
            isouter=True
        ).filter(
            (DocumentRecord.workspace_id == workspace_id) |
            (DocumentRecord.is_global == True)
        ).order_by(DocumentRecord.upload_date.desc()).all()

    result = []

    # 4. ğŸ”¥ ç°åœ¨ docs é‡Œçš„æ¯ä¸€é¡¹éƒ½æ˜¯ä¸€ä¸ªå…ƒç»„ (DocumentRecordå¯¹è±¡, éƒ¨é—¨åç§°å­—ç¬¦ä¸²)
    # æ‰€ä»¥è¿™é‡Œå¯ä»¥è§£åŒ…äº†
    for doc, ws_name in docs:

        # ä¼˜åŒ–ï¼šå¤„ç† Global æ–‡æ¡£ Join ä¸åˆ°çš„æƒ…å†µ
        final_ws_name = ws_name
        if not final_ws_name:
            if doc.is_global or doc.workspace_id == 'global':
                final_ws_name = "å…¬å…±çŸ¥è¯†åº“"
            else:
                final_ws_name = "æœªçŸ¥éƒ¨é—¨"

        # æŸ¥è¯¢ä¸Šä¼ è€…ä¿¡æ¯ (è¿™é‡Œè¿˜æ˜¯ä¼šæœ‰ N+1 é—®é¢˜ï¼Œä½†æš‚ä¸”ä¿æŒåŸæ ·)
        uploader_name = "Unknown"
        if doc.uploader_id:
            u = db.query(User).filter(User.id == doc.uploader_id).first()
            if u: uploader_name = u.username

        result.append({
            "id": str(doc.id),
            "name": doc.filename,
            "size": doc.file_size,
            "status": "indexed",
            "date": doc.upload_date.strftime("%Y-%m-%d"),
            "isGlobal": doc.is_global,
            "uploader_id": doc.uploader_id,
            "uploader_name": uploader_name,
            "workspace_name": final_ws_name  # ä½¿ç”¨å¤„ç†åçš„åç§°
        })

    return result